{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm\n",
    "import time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install libraries\n",
    "\n",
    "#%pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "#%pip install transformers datasets evaluate -q\n",
    "#%pip install pyarrow tree-sitter-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load dataset \n",
    "\n",
    "# Split the dataset into train, test, and validation sections\n",
    "test_dataset = load_dataset(\"linyalan/python-bugs-name-noise-1\", split=\"train[0:100]\")\n",
    "train_dataset = load_dataset(\"linyalan/python-bugs-name-noise-1\", split=\"train[100:800]\")\n",
    "validation_dataset = load_dataset(\"linyalan/python-bugs-name-noise-1\", split=\"train[800:1000]\")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'test': test_dataset,\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset\n",
    "})\n",
    "#print(dataset)\n",
    "#print(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Existing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32100, 512)"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load existing model & tokenizer\n",
    "\n",
    "model_checkpoint = \"Salesforce/codet5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of parameters:  60492288\n",
      "T5Config {\n",
      "  \"_attn_implementation_autoset\": true,\n",
      "  \"architectures\": [\n",
      "    \"T5ForConditionalGeneration\"\n",
      "  ],\n",
      "  \"bos_token_id\": 1,\n",
      "  \"classifier_dropout\": 0.0,\n",
      "  \"d_ff\": 2048,\n",
      "  \"d_kv\": 64,\n",
      "  \"d_model\": 512,\n",
      "  \"decoder_start_token_id\": 0,\n",
      "  \"dense_act_fn\": \"relu\",\n",
      "  \"dropout_rate\": 0.1,\n",
      "  \"eos_token_id\": 2,\n",
      "  \"feed_forward_proj\": \"relu\",\n",
      "  \"gradient_checkpointing\": false,\n",
      "  \"id2label\": {\n",
      "    \"0\": \"LABEL_0\"\n",
      "  },\n",
      "  \"initializer_factor\": 1.0,\n",
      "  \"is_encoder_decoder\": true,\n",
      "  \"is_gated_act\": false,\n",
      "  \"label2id\": {\n",
      "    \"LABEL_0\": 0\n",
      "  },\n",
      "  \"layer_norm_epsilon\": 1e-06,\n",
      "  \"model_type\": \"t5\",\n",
      "  \"n_positions\": 512,\n",
      "  \"num_decoder_layers\": 6,\n",
      "  \"num_heads\": 8,\n",
      "  \"num_layers\": 6,\n",
      "  \"output_past\": true,\n",
      "  \"pad_token_id\": 0,\n",
      "  \"relative_attention_max_distance\": 128,\n",
      "  \"relative_attention_num_buckets\": 32,\n",
      "  \"task_specific_params\": {\n",
      "    \"summarization\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"length_penalty\": 2.0,\n",
      "      \"max_length\": 200,\n",
      "      \"min_length\": 30,\n",
      "      \"no_repeat_ngram_size\": 3,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"summarize: \"\n",
      "    },\n",
      "    \"translation_en_to_de\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to German: \"\n",
      "    },\n",
      "    \"translation_en_to_fr\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to French: \"\n",
      "    },\n",
      "    \"translation_en_to_ro\": {\n",
      "      \"early_stopping\": true,\n",
      "      \"max_length\": 300,\n",
      "      \"num_beams\": 4,\n",
      "      \"prefix\": \"translate English to Romanian: \"\n",
      "    }\n",
      "  },\n",
      "  \"torch_dtype\": \"float32\",\n",
      "  \"transformers_version\": \"4.51.3\",\n",
      "  \"use_cache\": true,\n",
      "  \"vocab_size\": 32100\n",
      "}\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# 2. Analyze existing model\n",
    "\n",
    "params = 0\n",
    "for p in model.parameters():\n",
    "    params += p.numel()\n",
    "\n",
    "print(\"Number of parameters: \",params)\n",
    "print(model.config)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenize the dataset\n",
    "\n",
    "def preprocess_function(dataset):\n",
    "    inputs = dataset[\"prompt_code\"]\n",
    "    targets = dataset[\"correct_code\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train = dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "valid = dataset[\"validation\"].map(preprocess_function, batched = True)\n",
    "test = dataset[\"test\"].map(preprocess_function, batched = True)\n",
    "#print(valid)\n",
    "#print(train)\n",
    "#print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:16<00:00,  1.27s/it]\n"
     ]
    }
   ],
   "source": [
    "# 3. Generate output from existing model\n",
    "\n",
    "all_inputs = test[\"prompt_code\"]\n",
    "batch_size = 8\n",
    "decoded_outputs = []\n",
    "\n",
    "model2 = model.to('cuda')\n",
    "\n",
    "for i in tqdm(range(0, len(all_inputs), batch_size)):\n",
    "    batch = all_inputs[i:i+batch_size]\n",
    "\n",
    "    # Tokenize batch\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model2.generate(**inputs, max_length=256)\n",
    "\n",
    "    # Decode each output\n",
    "    decoded_batch = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    decoded_outputs.extend(decoded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0034824889169004147, 'precisions': [0.5877890173410405, 0.344698388909704, 0.2625289128758674, 0.21284185493460167], 'brevity_penalty': 0.010676183234260015, 'length_ratio': 0.18051389070040433, 'translation_length': 2768, 'reference_length': 15334}\n"
     ]
    }
   ],
   "source": [
    "# 4. calculate BLEU score for existing model\n",
    "\n",
    "predictions = decoded_outputs\n",
    "references = test[\"correct_code\"]\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_score = bleu.compute(references=references, predictions=predictions)\n",
    "print(bleu_score)\n",
    "\n",
    "# score: 0.00348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buggy code: def child_relationships(self, pid, **kwargs):\n",
      "    \"\"\"https://familysearch.org/developers/docs/api/tree/Relationships_to_Children_resource\"\"\"\n",
      "    return self._add_query_params(\n",
      "        self.person_base + pid / '/child-relationships', kwargs)\n",
      "prediction: self.person_base\n",
      "target: def child_relationships(self, pid, **kwargs):\n",
      "    \"\"\"https://familysearch.org/developers/docs/api/tree/Relationships_to_Children_resource\"\"\"\n",
      "    return self._add_query_params(\n",
      "        self.person_base + pid + '/child-relationships', kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 5. Example prediction from existing model\n",
    "\n",
    "print(\"buggy code:\",test[\"prompt_code\"][8])\n",
    "print(\"prediction:\",decoded_outputs[8])\n",
    "print(\"target:\",test[\"correct_code\"][8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 119,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rowan\\AppData\\Local\\Temp\\ipykernel_4196\\851539404.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 1. Define fine-tuning training arguments and create Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bugfixer-finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=0.00005,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=valid,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 120,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1750/1750 04:17, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.101200</td>\n",
       "      <td>0.038508</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.044700</td>\n",
       "      <td>0.028331</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.032400</td>\n",
       "      <td>0.025123</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.023100</td>\n",
       "      <td>0.024846</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.021400</td>\n",
       "      <td>0.024326</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1750, training_loss=0.08680290385654994, metrics={'train_runtime': 257.3359, 'train_samples_per_second': 13.601, 'train_steps_per_second': 6.8, 'total_flos': 236848152576000.0, 'train_loss': 0.08680290385654994, 'epoch': 5.0})"
      ]
     },
     "execution_count": 120,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Train the model\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bugfixer-finetuned\\\\tokenizer_config.json',\n",
       " './bugfixer-finetuned\\\\special_tokens_map.json',\n",
       " './bugfixer-finetuned\\\\vocab.json',\n",
       " './bugfixer-finetuned\\\\merges.txt',\n",
       " './bugfixer-finetuned\\\\added_tokens.json')"
      ]
     },
     "execution_count": 121,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Save the model and tokenizer so you don't have to retrain\n",
    "\n",
    "save_path = \"./bugfixer-finetuned\"\n",
    "\n",
    "trainer.save_model(save_path)\n",
    "\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 122,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load saved model and tokenizer if applicable\n",
    "\n",
    "save_path = \"./bugfixer-finetuned\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(save_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def place_types(self):\n",
      "    \"\"\"https://familysearch.org/developers/docs/api/places/Place_Types_resource\"\"\"\n",
      "    return self.places_base / \"types\"\n",
      "<pad><s>def place_types(self):\n",
      "    \"\"\"https://familysearch.org/developers/docs/api/places/Place_Types_resource\"\"\"\n",
      "    return self.places_base + \"types\"</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32100, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 123,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Small test with fine-tuned model\n",
    "\n",
    "model2 = model.to('cuda')\n",
    "input_code = test[\"prompt_code\"][0]\n",
    "print(test[\"prompt_code\"][0])\n",
    "inputs = tokenizer(input_code, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "outputs = model2.generate(**inputs.to('cuda'), max_length=256)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:52<00:00,  4.07s/it]\n"
     ]
    }
   ],
   "source": [
    "# 6. Full test with fine-tuned model\n",
    "\n",
    "all_inputs = test[\"prompt_code\"]\n",
    "batch_size = 8\n",
    "decoded_outputs = []\n",
    "\n",
    "for i in tqdm(range(0, len(all_inputs), batch_size)):\n",
    "    batch = all_inputs[i:i+batch_size]\n",
    "\n",
    "    # Tokenize batch\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model2.generate(**inputs, max_length=256)\n",
    "\n",
    "    # Decode each output\n",
    "    decoded_batch = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    decoded_outputs.extend(decoded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: def place_types(self):\n",
      "    \"\"\"https://familysearch.org/developers/docs/api/places/Place_Types_resource\"\"\"\n",
      "    return self.places_base / \"types\"\n",
      "Target: def place_types(self):\n",
      "    \"\"\"https://familysearch.org/developers/docs/api/places/Place_Types_resource\"\"\"\n",
      "    return self.places_base + \"types\"\n",
      "Prediction: def place_types(self):\n",
      "    \"\"\"https://familysearch.org/developers/docs/api/places/Place_Types_resource\"\"\"\n",
      "    return self.places_base + \"types\"\n",
      "Input: def zset_score_pairs(response, **options):\n",
      "    \"\"\"\n",
      "    If ``withscores`` is specified in the options, return the response as\n",
      "    a list of (value, score) pairs\n",
      "    \"\"\"\n",
      "    if not response and not options['withscores']:\n",
      "        return response\n",
      "    score_cast_func = options.get('score_cast_func', float)\n",
      "    it = iter(response)\n",
      "    return list(izip(it, imap(score_cast_func, it)))\n",
      "Target: def zset_score_pairs(response, **options):\n",
      "    \"\"\"\n",
      "    If ``withscores`` is specified in the options, return the response as\n",
      "    a list of (value, score) pairs\n",
      "    \"\"\"\n",
      "    if not response or not options['withscores']:\n",
      "        return response\n",
      "    score_cast_func = options.get('score_cast_func', float)\n",
      "    it = iter(response)\n",
      "    return list(izip(it, imap(score_cast_func, it)))\n",
      "Prediction: def zset_score_pairs(response, **options):\n",
      "    \"\"\"\n",
      "    If ``withscores`` is specified in the options, return the response as\n",
      "    a list of (value, score) pairs\n",
      "    \"\"\"\n",
      "    if not response and not options['withscores']:\n",
      "        return response\n",
      "    score_cast_func = options.get('score_cast_func', float)\n",
      "    it = iter(response)\n",
      "    return list(izip(it, imap(score_cast_func, it)))\n",
      "Input: def setUp(self):\n",
      "    \"\"\"Set up a blank temp database before each test\"\"\"\n",
      "    basedir = os.path.abspath(os.path.dirname(__file__))\n",
      "    app.config['TESTING'] = True\n",
      "    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' / \\\n",
      "                                            os.path.join(basedir, TEST_DB)\n",
      "    self.app = app.test_client()\n",
      "    db.create_all()\n",
      "Target: def setUp(self):\n",
      "    \"\"\"Set up a blank temp database before each test\"\"\"\n",
      "    basedir = os.path.abspath(os.path.dirname(__file__))\n",
      "    app.config['TESTING'] = True\n",
      "    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' + \\\n",
      "                                            os.path.join(basedir, TEST_DB)\n",
      "    self.app = app.test_client()\n",
      "    db.create_all()\n",
      "Prediction: def setUp(self):\n",
      "    \"\"\"Set up a blank temp database before each test\"\"\"\n",
      "    basedir = os.path.abspath(os.path.dirname(__file__))\n",
      "    app.config['TESTING'] = True\n",
      "    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' / \\\n",
      "                                            os.path.join(basedir, TEST_DB)\n",
      "    self.app = app.test_client()\n",
      "    db.create_all()\n"
     ]
    }
   ],
   "source": [
    "# 7. 3 examples of model output\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Input:\",test[\"prompt_code\"][i])\n",
    "    print(\"Target:\",test[\"correct_code\"][i])\n",
    "    print(f\"Prediction: {decoded_outputs[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.7097070588633069, 'precisions': [0.9898937548587717, 0.9842293282216608, 0.9790806012129736, 0.9740179125653986], 'brevity_penalty': 0.7228722544230599, 'length_ratio': 0.7549889135254989, 'translation_length': 11577, 'reference_length': 15334}\n"
     ]
    }
   ],
   "source": [
    "# 8. Calculate BLEU score for fine-tuned model\n",
    "\n",
    "predictions = decoded_outputs\n",
    "references = test[\"correct_code\"]\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_score = bleu.compute(references=references, predictions=predictions)\n",
    "print(bleu_score)\n",
    "\n",
    "# bleu score: 0.7097"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
