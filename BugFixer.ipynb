{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 1: Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "from transformers import AutoModelForSeq2SeqLM\n",
    "from transformers import RobertaTokenizer\n",
    "from transformers import TrainingArguments, Trainer\n",
    "from transformers import EarlyStoppingCallback\n",
    "from datasets import load_dataset\n",
    "from datasets import DatasetDict\n",
    "import torch\n",
    "import evaluate\n",
    "from tqdm import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 1. Install libraries\n",
    "\n",
    "#%pip install torch==2.5.1 torchvision==0.20.1 torchaudio==2.5.1 --index-url https://download.pytorch.org/whl/cu124\n",
    "#%pip install transformers datasets evaluate -q\n",
    "#%pip install pyarrow tree-sitter-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Load dataset \n",
    "\n",
    "# Split the dataset into train, test, and validation sections\n",
    "test_dataset = load_dataset(\"linyalan/python-bugs-name-noise-1\", split=\"train[0:100]\")\n",
    "train_dataset = load_dataset(\"linyalan/python-bugs-name-noise-1\", split=\"train[100:800]\")\n",
    "validation_dataset = load_dataset(\"linyalan/python-bugs-name-noise-1\", split=\"train[800:1000]\")\n",
    "\n",
    "dataset = DatasetDict({\n",
    "    'test': test_dataset,\n",
    "    'train': train_dataset,\n",
    "    'validation': validation_dataset\n",
    "})\n",
    "#print(dataset)\n",
    "#print(dataset[\"train\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 2: Existing Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Embedding(32100, 512)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 1. Load existing model & tokenizer\n",
    "\n",
    "model_checkpoint = \"Salesforce/codet5-small\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(model_checkpoint)\n",
    "\n",
    "tokenizer = RobertaTokenizer.from_pretrained(model_checkpoint)\n",
    "\n",
    "model.resize_token_embeddings(len(tokenizer))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 2. Tokenize the dataset\n",
    "\n",
    "def preprocess_function(dataset):\n",
    "    inputs = dataset[\"prompt_code\"]\n",
    "    targets = dataset[\"correct_code\"]\n",
    "    model_inputs = tokenizer(inputs, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    labels = tokenizer(targets, max_length=256, truncation=True, padding=\"max_length\")\n",
    "    model_inputs[\"labels\"] = labels[\"input_ids\"]\n",
    "    return model_inputs\n",
    "\n",
    "train = dataset[\"train\"].map(preprocess_function, batched=True)\n",
    "valid = dataset[\"validation\"].map(preprocess_function, batched = True)\n",
    "test = dataset[\"test\"].map(preprocess_function, batched = True)\n",
    "#print(valid)\n",
    "#print(train)\n",
    "#print(test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:15<00:00,  1.22s/it]\n"
     ]
    }
   ],
   "source": [
    "# 3. Generate output from existing model\n",
    "\n",
    "all_inputs = test[\"prompt_code\"]\n",
    "batch_size = 8\n",
    "decoded_outputs = []\n",
    "\n",
    "model2 = model.to('cuda')\n",
    "\n",
    "for i in tqdm(range(0, len(all_inputs), batch_size)):\n",
    "    batch = all_inputs[i:i+batch_size]\n",
    "\n",
    "    # Tokenize batch\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model2.generate(**inputs, max_length=256)\n",
    "\n",
    "    # Decode each output\n",
    "    decoded_batch = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    decoded_outputs.extend(decoded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.0034824889169004147, 'precisions': [0.5877890173410405, 0.344698388909704, 0.2625289128758674, 0.21284185493460167], 'brevity_penalty': 0.010676183234260015, 'length_ratio': 0.18051389070040433, 'translation_length': 2768, 'reference_length': 15334}\n"
     ]
    }
   ],
   "source": [
    "# 4. calculate BLEU score for existing model\n",
    "\n",
    "predictions = decoded_outputs\n",
    "references = test[\"correct_code\"]\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_score = bleu.compute(references=references, predictions=predictions)\n",
    "print(bleu_score)\n",
    "\n",
    "# score: 0.00348"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "buggy code: def child_relationships(self, pid, **kwargs):\n",
      "    \"\"\"https://familysearch.org/developers/docs/api/tree/Relationships_to_Children_resource\"\"\"\n",
      "    return self._add_query_params(\n",
      "        self.person_base + pid / '/child-relationships', kwargs)\n",
      "prediction: self.person_base\n",
      "target: def child_relationships(self, pid, **kwargs):\n",
      "    \"\"\"https://familysearch.org/developers/docs/api/tree/Relationships_to_Children_resource\"\"\"\n",
      "    return self._add_query_params(\n",
      "        self.person_base + pid + '/child-relationships', kwargs)\n"
     ]
    }
   ],
   "source": [
    "# 5. Example prediction from existing model\n",
    "\n",
    "print(\"buggy code:\",test[\"prompt_code\"][8])\n",
    "print(\"prediction:\",decoded_outputs[8])\n",
    "print(\"target:\",test[\"correct_code\"][8])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Part 3: Fine-tuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\rowan\\AppData\\Local\\Temp\\ipykernel_4196\\1863318318.py:20: FutureWarning: `tokenizer` is deprecated and will be removed in version 5.0.0 for `Trainer.__init__`. Use `processing_class` instead.\n",
      "  trainer = Trainer(\n"
     ]
    }
   ],
   "source": [
    "# 1. Define fine-tuning training arguments and create Trainer\n",
    "\n",
    "training_args = TrainingArguments(\n",
    "    output_dir=\"./bugfixer-finetuned\",\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_dir=\"./logs\",\n",
    "    learning_rate=5e-5,\n",
    "    per_device_train_batch_size=2,\n",
    "    per_device_eval_batch_size=2,\n",
    "    num_train_epochs=5,\n",
    "    weight_decay=0.01,\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model=\"eval_loss\",\n",
    "    save_total_limit=2,\n",
    "    logging_steps=100,\n",
    "    push_to_hub=False,\n",
    ")\n",
    "\n",
    "trainer = Trainer(\n",
    "    model=model,\n",
    "    args=training_args,\n",
    "    train_dataset=train,\n",
    "    eval_dataset=valid,\n",
    "    tokenizer=tokenizer,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='1750' max='1750' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [1750/1750 04:41, Epoch 5/5]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>0.010300</td>\n",
       "      <td>0.032018</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>0.006300</td>\n",
       "      <td>0.032591</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>0.006000</td>\n",
       "      <td>0.029678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.029162</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>0.003700</td>\n",
       "      <td>0.028645</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "There were missing keys in the checkpoint model loaded: ['encoder.embed_tokens.weight', 'decoder.embed_tokens.weight', 'lm_head.weight'].\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "TrainOutput(global_step=1750, training_loss=0.006436010794980186, metrics={'train_runtime': 282.1898, 'train_samples_per_second': 12.403, 'train_steps_per_second': 6.202, 'total_flos': 236848152576000.0, 'train_loss': 0.006436010794980186, 'epoch': 5.0})"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 2. Train the model\n",
    "\n",
    "trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "('./bugfixer-finetuned\\\\tokenizer_config.json',\n",
       " './bugfixer-finetuned\\\\special_tokens_map.json',\n",
       " './bugfixer-finetuned\\\\vocab.json',\n",
       " './bugfixer-finetuned\\\\merges.txt',\n",
       " './bugfixer-finetuned\\\\added_tokens.json')"
      ]
     },
     "execution_count": 84,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 3. Save the model and tokenizer so you don't have to retrain\n",
    "\n",
    "save_path = \"./bugfixer-finetuned\"\n",
    "\n",
    "trainer.save_model(save_path)\n",
    "\n",
    "tokenizer.save_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 4. Load saved model and tokenizer if applicable\n",
    "\n",
    "save_path = \"./bugfixer-finetuned\"\n",
    "model = AutoModelForSeq2SeqLM.from_pretrained(save_path)\n",
    "tokenizer = RobertaTokenizer.from_pretrained(save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "def place_types(self):\n",
      "    \"\"\"https://familysearch.org/developers/docs/api/places/Place_Types_resource\"\"\"\n",
      "    return self.places_base / \"types\"\n",
      "<pad><s>def place_types(self):\n",
      "    \"\"\"https://familysearch.org/developers/docs/api/places/Place_Types_resource\"\"\"\n",
      "    return self.places_base + \"types\"</s>\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "T5ForConditionalGeneration(\n",
       "  (shared): Embedding(32100, 512)\n",
       "  (encoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (decoder): T5Stack(\n",
       "    (embed_tokens): Embedding(32100, 512)\n",
       "    (block): ModuleList(\n",
       "      (0): T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (relative_attention_bias): Embedding(32, 8)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "      (1-5): 5 x T5Block(\n",
       "        (layer): ModuleList(\n",
       "          (0): T5LayerSelfAttention(\n",
       "            (SelfAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (1): T5LayerCrossAttention(\n",
       "            (EncDecAttention): T5Attention(\n",
       "              (q): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (k): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (v): Linear(in_features=512, out_features=512, bias=False)\n",
       "              (o): Linear(in_features=512, out_features=512, bias=False)\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "          (2): T5LayerFF(\n",
       "            (DenseReluDense): T5DenseActDense(\n",
       "              (wi): Linear(in_features=512, out_features=2048, bias=False)\n",
       "              (wo): Linear(in_features=2048, out_features=512, bias=False)\n",
       "              (dropout): Dropout(p=0.1, inplace=False)\n",
       "              (act): ReLU()\n",
       "            )\n",
       "            (layer_norm): T5LayerNorm()\n",
       "            (dropout): Dropout(p=0.1, inplace=False)\n",
       "          )\n",
       "        )\n",
       "      )\n",
       "    )\n",
       "    (final_layer_norm): T5LayerNorm()\n",
       "    (dropout): Dropout(p=0.1, inplace=False)\n",
       "  )\n",
       "  (lm_head): Linear(in_features=512, out_features=32100, bias=False)\n",
       ")"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# 5. Small test with fine-tuned model\n",
    "\n",
    "model2 = model.to('cuda')\n",
    "input_code = test[\"prompt_code\"][0]\n",
    "print(test[\"prompt_code\"][0])\n",
    "inputs = tokenizer(input_code, return_tensors=\"pt\", padding=True, truncation=True)\n",
    "outputs = model2.generate(**inputs.to('cuda'), max_length=256)\n",
    "print(tokenizer.decode(outputs[0]))\n",
    "model2.eval()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 13/13 [00:47<00:00,  3.66s/it]\n"
     ]
    }
   ],
   "source": [
    "# 6. Full test with fine-tuned model\n",
    "\n",
    "all_inputs = test[\"prompt_code\"]\n",
    "batch_size = 8\n",
    "decoded_outputs = []\n",
    "\n",
    "for i in tqdm(range(0, len(all_inputs), batch_size)):\n",
    "    batch = all_inputs[i:i+batch_size]\n",
    "\n",
    "    # Tokenize batch\n",
    "    inputs = tokenizer(batch, return_tensors=\"pt\", padding=True, truncation=True, max_length=512)\n",
    "    inputs = {k: v.to('cuda') for k, v in inputs.items()}\n",
    "\n",
    "    with torch.no_grad():\n",
    "        outputs = model2.generate(**inputs, max_length=256)\n",
    "\n",
    "    # Decode each output\n",
    "    decoded_batch = tokenizer.batch_decode(outputs, skip_special_tokens=True)\n",
    "    decoded_outputs.extend(decoded_batch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Input: def place_types(self):\n",
      "    \"\"\"https://familysearch.org/developers/docs/api/places/Place_Types_resource\"\"\"\n",
      "    return self.places_base / \"types\"\n",
      "Target: def place_types(self):\n",
      "    \"\"\"https://familysearch.org/developers/docs/api/places/Place_Types_resource\"\"\"\n",
      "    return self.places_base + \"types\"\n",
      "Prediction: def place_types(self):\n",
      "    \"\"\"https://familysearch.org/developers/docs/api/places/Place_Types_resource\"\"\"\n",
      "    return self.places_base + \"types\"\n",
      "Input: def zset_score_pairs(response, **options):\n",
      "    \"\"\"\n",
      "    If ``withscores`` is specified in the options, return the response as\n",
      "    a list of (value, score) pairs\n",
      "    \"\"\"\n",
      "    if not response and not options['withscores']:\n",
      "        return response\n",
      "    score_cast_func = options.get('score_cast_func', float)\n",
      "    it = iter(response)\n",
      "    return list(izip(it, imap(score_cast_func, it)))\n",
      "Target: def zset_score_pairs(response, **options):\n",
      "    \"\"\"\n",
      "    If ``withscores`` is specified in the options, return the response as\n",
      "    a list of (value, score) pairs\n",
      "    \"\"\"\n",
      "    if not response or not options['withscores']:\n",
      "        return response\n",
      "    score_cast_func = options.get('score_cast_func', float)\n",
      "    it = iter(response)\n",
      "    return list(izip(it, imap(score_cast_func, it)))\n",
      "Prediction: def zset_score_pairs(response, **options):\n",
      "    \"\"\"\n",
      "    If ``withscores`` is specified in the options, return the response as\n",
      "    a list of (value, score) pairs\n",
      "    \"\"\"\n",
      "    if not response and not options['withscores']:\n",
      "        return response\n",
      "    score_cast_func = options.get('score_cast_func', float)\n",
      "    it = iter(response)\n",
      "    return list(izip(it, imap(score_cast_func, it)))\n",
      "Input: def setUp(self):\n",
      "    \"\"\"Set up a blank temp database before each test\"\"\"\n",
      "    basedir = os.path.abspath(os.path.dirname(__file__))\n",
      "    app.config['TESTING'] = True\n",
      "    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' / \\\n",
      "                                            os.path.join(basedir, TEST_DB)\n",
      "    self.app = app.test_client()\n",
      "    db.create_all()\n",
      "Target: def setUp(self):\n",
      "    \"\"\"Set up a blank temp database before each test\"\"\"\n",
      "    basedir = os.path.abspath(os.path.dirname(__file__))\n",
      "    app.config['TESTING'] = True\n",
      "    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' + \\\n",
      "                                            os.path.join(basedir, TEST_DB)\n",
      "    self.app = app.test_client()\n",
      "    db.create_all()\n",
      "Prediction: def setUp(self):\n",
      "    \"\"\"Set up a blank temp database before each test\"\"\"\n",
      "    basedir = os.path.abspath(os.path.dirname(__file__))\n",
      "    app.config['TESTING'] = True\n",
      "    app.config['SQLALCHEMY_DATABASE_URI'] = 'sqlite:///' + \\\n",
      "                                            os.path.join(basedir, TEST_DB)\n",
      "    self.app = app.test_client()\n",
      "    db.create_all()\n"
     ]
    }
   ],
   "source": [
    "# 7. 3 examples of model output\n",
    "\n",
    "for i in range(3):\n",
    "    print(\"Input:\",test[\"prompt_code\"][i])\n",
    "    print(\"Target:\",test[\"correct_code\"][i])\n",
    "    print(f\"Prediction: {decoded_outputs[i]}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'bleu': 0.7141648707151963, 'precisions': [0.9963645806284082, 0.9924037370121366, 0.9888135294635779, 0.9851595130187506], 'brevity_penalty': 0.7208859761257439, 'length_ratio': 0.7534237641841659, 'translation_length': 11553, 'reference_length': 15334}\n"
     ]
    }
   ],
   "source": [
    "# 8. Calculate BLEU score for fine-tuned model\n",
    "\n",
    "predictions = decoded_outputs\n",
    "references = test[\"correct_code\"]\n",
    "\n",
    "bleu = evaluate.load(\"bleu\")\n",
    "bleu_score = bleu.compute(references=references, predictions=predictions)\n",
    "print(bleu_score)\n",
    "\n",
    "# bleu score: 0.7142"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
